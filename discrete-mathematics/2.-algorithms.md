# 2. Algorithms

## Algorithms

An **algorithm** is a finite sequence of precise instructions for performing a computation or for solving a problem.

Searching: linear search, binary search.

Sorting: bubble sorting, insertion sorting, etc.

Many algorithms we will study in this book are designed to solve **optimization problems** 最优化问题. Algorithms that make what seems to be the “best” choice at each step are called **greedy algorithms** 贪婪算法.

## The Growth of Functions

One of the advantages of using **big-O notation**, which we introduce in this section, is that we can estimate the growth of a function without worrying about constant multipliers or smaller order terms. This means that, using big- O notation, we do not have to worry about the hardware and software used to implement an algorithm. Furthermore, using big-O notation, we can assume that the different operations used in an algorithm take the same time, which simplifies the analysis considerably.

Let _f_ and _g_ be functions from the set of integers or the set of real numbers to the set of real numbers. We say that _f\(x\)_ is _O\(g\(x\)\)_ if there are constants C and k such that \|f\(x\)\| ≤ C\|g\(x\)\| whenever x &gt; k. \[This is read as “f \(x\) is big-oh of g\(x\).”\]

When big-O notation is used, the function g in the relationship f \(x\) is O\(g\(x\)\) is chosen to be as small as possible.

Polynomials 多项式 can often be used to estimate the **growth of functions**. The leading term of a polynomial dominates its growth.

Let $$f(x)=a_nx^n+a_{n-1}x^{n-1}+...+a_1x+a_0$$, where $$a_0$$ , $$a_1$$ , . . . , $$a_{n-1}$$ , $$a_n$$ are real numbers. Then $$f(x)$$ is $$O(x^n)$$ .

1 + 2+· · ·+n ≤ n + n+· · ·+n = $$n^2 $$.

The factorial function 阶乘 f\(n\) = n! is defined by n! = 1 · 2 · 3 · · · · · n whenever n is a positive integer, and 0! = 1.

Note that the function n! grows rapidly. For instance, 20! = 2,432,902,008,176,640,000.

n! = 1 · 2 · 3 · · · · · n ≤ n · n · n · · · · · n = $$n^n$$ .

Taking logarithms of both sides: $$log\ n! \leq log\ n^n = n\ log\ n$$ .

Big-O notation is used to estimate the number of operations needed to solve a problem using a specified procedure or algorithm. The functions used in these estimates often include the following: 1, log n, n, n log n, $$n^2$$ , $$2^n$$ , n!

## Complexity of Algorithms

Analysis of the time required to solve a problem of a particular size involves the **time complexity** of the algorithms. Because data structures are not dealt with in detail in this book, **space complexity** will not be considered.

**Worst-case** analysis tells us how many operations an algorithm requires to guarantee that it will produce a solution. The average number of operations used to solve the problem over all possible inputs of a given size is found in **average-case** analysis.

Greedy algorithms provide an example of an **algorithmic paradigm**, that is, a general approach based on a particular concept that can be used to construct algorithms for solving a variety of problems.

Brute force is an important, and basic, algorithmic paradigm. In a **brute-force** algorithm 蛮力算法, a problem is solved in the most straightforward manner based on the statement of the problem and the definitions of terms.

![](../.gitbook/assets/screen-shot-2018-09-24-at-13.31.35.png)

A problem that is solvable using an algorithm with polynomial worst-case complexity is called **tractable**. The situation is much worse for problems that cannot be solved using an algorithm with worst-case polynomial time complexity. Such problems are called **intractable**.

Some problems even exist for which it can be shown that no algorithm exists for solving them. Such problems are called **unsolvable**. The first proof that there are unsolvable problems was provided by the great English mathematician and computer scientist Alan Turing when he showed that the **halting problem** is unsolvable.



